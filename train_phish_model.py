# train_phish_model.py
import os
import pandas as pd
import numpy as np
import re
import joblib
import tldextract
from urllib.parse import urlparse
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# -------- feature extractor for a URL --------
def is_ip_in_host(host):
    # simple IPv4 check (exact match)
    return bool(re.match(r'^\d{1,3}(\.\d{1,3}){3}$', host))

def count_char(s, ch):
    return s.count(ch) if isinstance(s, str) else 0

def extract_features_from_url(url):
    # normalize
    if not isinstance(url, str):
        url = str(url)
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url
    parsed = urlparse(url)
    host = parsed.hostname or ''
    path = parsed.path or ''
    query = parsed.query or ''
    scheme = parsed.scheme or ''
    ext = tldextract.extract(host)

    features = {}
    features['url_len'] = len(url)
    features['host_len'] = len(host)
    features['path_len'] = len(path)
    features['count_dots'] = count_char(host, '.')
    features['count_hyphen'] = count_char(host, '-')
    features['count_at'] = count_char(url, '@')
    features['count_qmark'] = count_char(url, '?')
    features['count_eq'] = count_char(url, '=')
    features['count_underscore'] = count_char(url, '_')
    features['has_ip'] = 1 if is_ip_in_host(host) else 0
    features['is_https'] = 1 if scheme == 'https' else 0
    # TLD suspicious length (e.g., long autogenerated tlds)
    features['tld_len'] = len(ext.suffix or '')
    # presence of suspicious words
    suspicious_words = ['login', 'secure', 'bank', 'verify', 'update', 'free', 'signin', 'account']
    url_lower = url.lower()
    features['suspicious_word_count'] = sum(1 for w in suspicious_words if w in url_lower)
    # token counts
    features['num_path_tokens'] = len([tok for tok in path.split('/') if tok])
    features['num_query_tokens'] = len([tok for tok in query.split('&') if tok])
    # dot to length ratio
    features['dots_per_len'] = features['count_dots'] / max(1, features['host_len'])
    return features

# -------- load dataset (customize as required) --------
csv_path = 'dataset_phishing.csv'  # change path if needed
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"Dataset file not found: {csv_path}")

df = pd.read_csv(csv_path)

# Try common column name variations for labels and urls
possible_label_cols = ['label', 'Label', 'phishing', 'Phishing', 'class', 'target', 'status']
possible_url_cols = ['url', 'URL', 'link', 'Link', 'domain']

label_col = next((c for c in possible_label_cols if c in df.columns), None)
url_col = next((c for c in possible_url_cols if c in df.columns), None)

if url_col is None:
    raise KeyError(f"No URL column found. Expected one of: {possible_url_cols}. Columns present: {list(df.columns)}")

if label_col is None:
    raise KeyError(f"No label column found. Expected one of: {possible_label_cols}. Columns present: {list(df.columns)}")

# Normalize labels robustly
def normalize_label(val):
    # handle NaN
    if pd.isna(val):
        return 0
    # strings
    if isinstance(val, str):
        v = val.strip().lower()
        if 'phish' in v or v == '1' or v == 'malicious' or v == 'bad':
            return 1
        if v in ('0', 'legit', 'legitimate', 'benign', 'safe', 'good'):
            return 0
        # try to parse int from string
        try:
            vi = int(v)
            return 1 if vi == 1 else 0
        except:
            return 0
    # numbers
    try:
        vi = int(val)
        # common dataset conventions: 1 -> phishing, 0 -> legitimate, -1 sometimes used -> legitimate
        if vi == 1:
            return 1
        return 0
    except:
        return 0

df['label'] = df[label_col].apply(normalize_label)
df['url_for_feature'] = df[url_col].astype(str)

# sanity check counts
class_counts = df['label'].value_counts().to_dict()
print("Label distribution (after normalization):", class_counts)

# create features
feat_rows = []
labels = []
for idx, row in df.iterrows():
    url = row['url_for_feature']
    if pd.isna(url) or str(url).strip() == '':
        continue
    feats = extract_features_from_url(url)
    feat_rows.append(feats)
    labels.append(int(row['label']))

if len(feat_rows) == 0:
    raise ValueError("No valid URLs found to extract features from.")

X = pd.DataFrame(feat_rows).fillna(0)
y = np.array(labels)

# train/test split
# Avoid stratify when y has only one class (train_test_split will error)
unique_labels = np.unique(y)
stratify_arg = y if len(unique_labels) > 1 else None
if stratify_arg is None:
    print("Warning: only one class present in labels; proceeding without stratify. Model may not be meaningful.")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify_arg
)

# model (small and fast)
clf = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)
clf.fit(X_train, y_train)

# eval
y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# save the model and the feature columns (ensure directory exists)
os.makedirs('models', exist_ok=True)
joblib.dump({'model': clf, 'feature_cols': list(X.columns)}, 'models/phish_detector.joblib')
print("Saved model to models/phish_detector.joblib")



